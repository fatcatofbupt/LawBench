datasets=[
    dict(abbr='lawbench-1-1-article_recitation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_1_1')),
        index='1-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-1-2-knowledge_question_answering-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_1_2')),
        index='1-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-1-document_proofreading-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_1')),
        index='2-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-2-dispute_focus_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_2')),
        index='2-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-3-marital_disputes_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_3')),
        index='2-3',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-4-issue_topic_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_4')),
        index='2-4',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-5-reading_comprehension-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_5')),
        index='2-5',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-6-named_entity_recognition-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_6')),
        index='2-6',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-7-opinion_summarization-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_7')),
        index='2-7',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-8-argument_mining-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_8')),
        index='2-8',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-9-event_detection-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_9')),
        index='2-9',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-10-trigger_word_extraction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_10')),
        index='2-10',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-1-fact_based_article_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_1')),
        index='3-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-2-scene_based_article_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_2')),
        index='3-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-3-charge_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_3')),
        index='3-3',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-4-prison_term_prediction_wo_article-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_4')),
        index='3-4',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-5-prison_term_prediction_w_article-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_5')),
        index='3-5',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-6-case_analysis-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_6')),
        index='3-6',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-7-criminal_damages_calculation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_7')),
        index='3-7',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-8-consultation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_8')),
        index='3-8',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    ]
lawbench_datasets=[
    dict(abbr='lawbench-1-1-article_recitation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_1_1')),
        index='1-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-1-2-knowledge_question_answering-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_1_2')),
        index='1-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-1-document_proofreading-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_1')),
        index='2-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-2-dispute_focus_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_2')),
        index='2-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-3-marital_disputes_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_3')),
        index='2-3',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-4-issue_topic_identification-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_4')),
        index='2-4',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-5-reading_comprehension-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_5')),
        index='2-5',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-6-named_entity_recognition-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_6')),
        index='2-6',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-7-opinion_summarization-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_7')),
        index='2-7',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-8-argument_mining-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_8')),
        index='2-8',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-9-event_detection-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_9')),
        index='2-9',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-2-10-trigger_word_extraction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_2_10')),
        index='2-10',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-1-fact_based_article_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_1')),
        index='3-1',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-2-scene_based_article_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_2')),
        index='3-2',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-3-charge_prediction-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_3')),
        index='3-3',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-4-prison_term_prediction_wo_article-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_4')),
        index='3-4',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-5-prison_term_prediction_w_article-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_5')),
        index='3-5',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-6-case_analysis-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_6')),
        index='3-6',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-7-criminal_damages_calculation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_7')),
        index='3-7',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    dict(abbr='lawbench-3-8-consultation-0-shot',
        eval_cfg=dict(
            evaluator=dict(
                type='LawBenchEvaluator_3_8')),
        index='3-8',
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=1024,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{instruction}\n{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/lawbench/zero_shot',
        reader_cfg=dict(
            input_columns=[
                'instruction',
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.LawBenchDataset'),
    ]
legalbrain_qwen=[
    dict(batch_size=1,
        max_seq_len=4096,
        meta_template=dict(
            reserved_roles=[
                dict(api_role='SYSTEM',
                    role='SYSTEM'),
                ],
            round=[
                dict(api_role='HUMAN',
                    role='HUMAN'),
                dict(api_role='QWEN_BOT',
                    generate=False,
                    role='QWEN_BOT'),
                ]),
        path='legalbrain_api_qwen',
        query_per_second=1,
        type='opencompass.models.LegalBrainAPI_Qwen'),
    ]
models=[
    dict(batch_size=1,
        max_seq_len=4096,
        meta_template=dict(
            reserved_roles=[
                dict(api_role='SYSTEM',
                    role='SYSTEM'),
                ],
            round=[
                dict(api_role='HUMAN',
                    role='HUMAN'),
                dict(api_role='QWEN_BOT',
                    generate=False,
                    role='QWEN_BOT'),
                ]),
        path='legalbrain_api_qwen',
        query_per_second=1,
        type='opencompass.models.LegalBrainAPI_Qwen'),
    ]
work_dir='outputs/default/20241011_072651'